{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3f25180",
   "metadata": {},
   "source": [
    "# Σημειωματάριο πέμπτο: NLP Ετοιμάζοντας τα δεδομένα μας για την βαθύτερη ανάλυση"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff5c3ea",
   "metadata": {},
   "source": [
    "Σε αυτό το notebook ακολουθούμε όλα τα απαραίτητα βήματα προεπεξεργασίας των δεδομένων κειμένου (δηλαδή των \n",
    "tweets μας) πριν τα τροφοδοτήσουμε στην ανάλυσή μας.Δημιουργούμε ακόμη κάποιες νέες στήλες στο DataFrame που μπορεί να μας φανούν χρήσιμα στη συνέχεια (feature extraction). Είναι σημαντικό να σημειωθεί ότι τα βήματα αυτά πρέπει να εκτελεστούν στην σωστή σειρά ώστε να αποφύγουμε την απώλεια πληροφορίας ή την δημιουργία περισσότερου θορύβου στα δεδομένα μας. Για παράδειγμα ένα σημαντικό βήμα στην προεπεξεργασία κειμένου είναι η αφαίρεση των σημείων στίξης. Αυτό όμως πρέπει να γίνει αφού πρώτα διαχειριστούμε τα *#hashtags*, τα *em@ils* και τα *@mentions* γιατί αλλιώς από τη μία θα χάσουμε την πληροφορία που υπάρχει σε τέτοιου είδους μεταδεδομένα κι από την άλλη με την αφαίρεση των σημείων στίξης από αυτά, εκείνο που μένει ενδεχομένως να είναι θόρυβος.\n",
    " \n",
    "Μερικά άρθρα και online υλικό που συμβουλευτήκαμε για την υλοποίηση των βημάτων προεπεξεργασίας είναι τα ακόλουθα: \n",
    "- https://towardsdatascience.com/nlp-text-preprocessing-a-practical-guide-and-template-d80874676e79\n",
    "- https://www.udemy.com/course/nlp-in-python/\n",
    "- https://www.analyticsvidhya.com/blog/2021/08/a-friendly-guide-to-nlp-text-pre-processing-with-python-example/\n",
    "- https://machinelearningmastery.com/gentle-introduction-bag-words-model/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afafbadc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pycld2 in /home/terzisilias/anaconda3/lib/python3.9/site-packages (0.41)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: emot in /home/terzisilias/anaconda3/lib/python3.9/site-packages (3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting pyspellchecker\n",
      "  Downloading pyspellchecker-0.6.3-py3-none-any.whl (2.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.7 MB 372 kB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: pyspellchecker\n",
      "Successfully installed pyspellchecker-0.6.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U pycld2\n",
    "%pip install emot\n",
    "%pip install pyspellchecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39acf355",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "import unicodedata\n",
    "import unidecode\n",
    "import pycld2 as cld2\n",
    "import emot\n",
    "from spellchecker import SpellChecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1e57611",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.read_csv('Data/tweets/tweets_2021_with_users_matched.csv', lineterminator='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa12bb4",
   "metadata": {},
   "source": [
    "Θα δημιουργήσουμε μια νέα στήλη με όνομα *clean_text* όπου θα αποθηκεύονται τα tweets ύστερα από την εφαρμογή των βημάτων preprocessing. Αρχικά διατηρούμε και την στήλη των *tweets* όπου έχουμε τα πρωτότυπα κείμενα έτσι ώστε να μπορούμε να βλέπουμε τις διαφορές που προκύπτουν. Στο τέλος όμως, πριν από την ανάλυση, θα χρειαστεί να αφαιρεθούν.\n",
    "\n",
    "Στην συνέχεια ακολουθούν ένα ένα τα βήματα του preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec498ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1bd80aa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I love u rite me a txt'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(TextBlob('I leve u rite me a txt').correct())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81054088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting autocorrect\n",
      "  Downloading autocorrect-2.6.1.tar.gz (622 kB)\n",
      "\u001b[K     |████████████████████████████████| 622 kB 637 kB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: autocorrect\n",
      "  Building wheel for autocorrect (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for autocorrect: filename=autocorrect-2.6.1-py3-none-any.whl size=622381 sha256=c93c0443c1555472d313532f47aa4eca9b4109ca64fef0bcedbbbf4a91a51c9a\n",
      "  Stored in directory: /home/terzisilias/.cache/pip/wheels/ab/0f/23/3c010c3fd877b962146e7765f9e9b08026cac8b035094c5750\n",
      "Successfully built autocorrect\n",
      "Installing collected packages: autocorrect\n",
      "Successfully installed autocorrect-2.6.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install autocorrect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35326fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex\n",
    "RE_BAD_CHARS = regex.compile(r\"\\p{Cc}|\\p{Cs}\")\n",
    "def remove_bad_chars(text):\n",
    "    # https://github.com/mikemccand/chromium-compact-language-detector/issues/22\n",
    "    return RE_BAD_CHARS.sub(\"\", text)\n",
    "\n",
    "def isEnglish(tweet):\n",
    "    \"\"\"\n",
    "    Έλεγχος για το εάν η γλώσσα του κειμένου του tweet είναι η αγγλική. \n",
    "    \n",
    "        Παράμετροι:\n",
    "            tweets (str): Το κείμενο του tweet\n",
    "            \n",
    "        Επιστρέφει:\n",
    "            True εάν η μετάφραση που επέστρεψε η βιβλιοθήκη είναι έμπιστη και αν το πιο πολύ κείμενο είναι\n",
    "            στην αγγλική (οπότε έρχεται πρώτη στο tuple), και εάν το ποσοστό της Αγγλικής γλώσσας \n",
    "            στο κείμενο είναι μεγαλύτερο από 90%. (Επιλέχθηκε αυτό το ποσοστό καθώς \n",
    "            σε περιπτώσεις μικρού κειμένου ακόμα και αγγλική να είναι \n",
    "            η γλώσσα, η βιβλιοθήκη μπορεί να επιστρέψει unknown ή χαμηλό σχετικά ποσοστό.)\n",
    "            \n",
    "            False σε κάθε άλλη περίπτωση έτσι ώστε να είμαστε σίγουροι ότι δεν θα κρατήσουμε στο κείμενο προτάσεις\n",
    "            από άλλες γλώσσες.\n",
    "    \"\"\"\n",
    "    \n",
    "    tweet = remove_bad_chars(tweet)    \n",
    "    isReliable, textBytesFound, details = cld2.detect(tweet)\n",
    "    if isReliable and details[0][0] == 'ENGLISH' and details[0][2] > 90:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365f805d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''IT Risk:\n",
    "Red Hat.python38:3.8 and python38-devel:3.8に複数の脆弱性 -1/2\n",
    "コードやコマンドを実行される\n",
    "機密データにアクセスされる\n",
    "クロスサイトスクリプティング\n",
    "機密データにアクセスされる\n",
    "CVE-2021-42771 CVE-2021-33503 CVE-2021-29921 CVE-2021-28957 CVE-2021-23336 CVE-2021-20095'''\n",
    "cld2.detect(text, returnVectors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb3dba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "emot_obj = emot.core.emot() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b555c5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_emoticons(text):\n",
    "    emoticons_found = emot_obj.emoticons(text)['value']\n",
    "    emoticons_meaning = emot_obj.emoticons(text)['mean']\n",
    "    for emoticon in emoticons_found:\n",
    "        text = text.replace(emoticon, emoticons_meaning[emoticons_found.index(emoticon)])\n",
    "    return text\n",
    "                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33865738",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_emojis(text):\n",
    "    emojis_found = emot_obj.emoji(text)['value']\n",
    "    emojis_meaning = emot_obj.emoji(text)['mean']\n",
    "    for emoji in emojis_found:\n",
    "        text = text.replace(emoji, emojis_meaning[emojis_found.index(emoji)].replace(\":\",\" \"))\n",
    "    return text\n",
    "                      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7b30bb",
   "metadata": {},
   "source": [
    "Δες αυτά παρακάτω. Αυτό σημαίνει ότι πρέπει να διαγράψουμε τους συνδέσμους πρώτα και μετά τα emoticons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec253d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '🥰 failure 🚨 :)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b77678",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_with_emoticon = 'hi there:(sddfgdf)  :D'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc16f58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lowercase\n",
    "tweets['clean_text'] = tweets['text'].apply(lambda tweet: tweet.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cecde7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Τακτοποιούμε την νέα στήλη όπου θα αποθηκεύουμε το κείμενο καθώς περνάει από τα διάφορα στάδια προεπεξεργασίας\n",
    "\n",
    "tweets = tweets.reindex(columns = ['id', 'text', 'clean_text', 'matched_CVE_IDs', 'created_at', 'retweet_count',\n",
    "       'reply_count', 'like_count', 'quote_count', 'author_id', 'author_name',\n",
    "       'author_username', 'author_description', 'author_followers_count',\n",
    "       'author_following_count', 'author_tweet_count', 'author_listed_count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c195ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tweets[tweets['clean_text'].apply(lambda tweet: not isEnglish(tweet))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abae52b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Δημιουργία νέας στήλης όπου αποθηκεύουμε το μέγεθος του tweet σε χαρακτήρες\n",
    "\n",
    "tweets['size_of_tweet_in_characters'] = tweets['text'].apply(lambda tweet: len(tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02e493a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Διαχείριση υπερσυνδέσμων.\n",
    "# https://stackoverflow.com/a/54086404\n",
    "\n",
    "# Πρώτα τους παίρνουμε από το text και τους κρατάμε σε μια νέα στήλη\n",
    "url_pattern = '''(?:(?:https?|ftp):\\/\\/|\\b(?:[a-z\\d]+\\.))(?:(?:[^\\s()<>]+|\\((?:[^\\s()<>]+|(?:\\([^\\s()<>]+\\)))?\\))+(?:\\((?:[^\\s()<>]+|(?:\\(?:[^\\s()<>]+\\)))?\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’]))?'''\n",
    "tweets['number_of_links_included'] = tweets['text'].apply(lambda tweet: len(re.findall(url_pattern, tweet)))\n",
    "tweets['links_included'] = tweets['text'].apply(lambda tweet: re.findall(url_pattern, tweet))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88360e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Και μετά τους διαγράφουμε από το clean_text\n",
    "tweets['clean_text'] = tweets['clean_text'].apply(lambda tweet: re.sub(url_pattern, '', tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da77d657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Διαχείριση emails\n",
    "\n",
    "# Πρώτα τα παίρνουμε από το text και τους κρατάμε σε μια νέα στήλη\n",
    "email_pattern = '([a-z0-9+._-]+@[a-z0-9+._-]+\\.[a-z+_-]+)'\n",
    "tweets['number_of_emails_included'] = tweets['text'].apply(lambda tweet: len(re.findall(email_pattern, tweet)))\n",
    "tweets['emails_included'] = tweets['text'].apply(lambda tweet: re.findall(email_pattern, tweet))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1de7654",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tweets[tweets['number_of_emails_included']>0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f636ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Και μετά τα διαγράφουμε από το clean_text\n",
    "tweets['clean_text'] = tweets['clean_text'].apply(lambda tweet: re.sub(email_pattern, '', tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1420d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Αφαίρεση αχρείαστων κενών (μπορεί να έχουν δημιουργηθεί ξανά)\n",
    "tweets['clean_text'] = tweets['clean_text'].apply(lambda tweet: ' '.join(tweet.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80df49c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tweets.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c87df3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Διαχείριση mentions\n",
    "\n",
    "# Το εφαρμόζουμε στο clean_text γιατί από εκεί έχουμε αφαιρέσει τα emails οπότε δεν υπάρχει κίμδυνος να πάρουμε \n",
    "# και email domains.\n",
    "# Πρώτα τα παίρνουμε και τα κρατάμε σε μια νέα στήλη\n",
    "# Χρησιμοποιούμε το replace για να αφαιρέσουμε τπν χαρακτήρα ':' που υπάρχει σε αρκετά mentions. Δεν μπορεί άλλωστε \n",
    "# να είναι τμήμα του username καθώς διαβάζουμε ότι \"A username can only contain alphanumeric \n",
    "# characters (letters A-Z, numbers 0-9) with the exception of underscores\"\n",
    "\n",
    "tweets['number_of_mentions_included'] = tweets['clean_text'].apply(lambda tweet: len([word for word in tweet.split() if word.startswith('@')]))\n",
    "tweets['mentions_included'] = tweets['clean_text'].apply(lambda tweet: [word.replace(':','') for word in tweet.split() if word.startswith('@')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333ab804",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Και μετά τα διαγράφουμε από το clean_text\n",
    "\n",
    "tweets['clean_text'] = tweets['clean_text'].apply(lambda tweet: ' '.join([word for word in tweet.split() if not word.startswith('@')]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab313cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets[tweets['number_of_mentions_included']>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00d352c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Διαγραφή των 'rt'\n",
    "\n",
    "tweets['clean_text'] = tweets['clean_text'].apply(lambda tweet: ' '.join([word for word in tweet.split() if not word == 'rt']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc020776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Διαχείριση hashtags\n",
    "# Πρώτα τα κρατάμε σε μια νέα στήλη \n",
    "\n",
    "tweets['number_of_hashtags_included'] = tweets['clean_text'].apply(lambda tweet: len([word for word in tweet.split() if word.startswith('#')]))\n",
    "tweets['hashtags_included'] = tweets['clean_text'].apply(lambda tweet: [word for word in tweet.split() if word.startswith('#')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09dbc565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Και μετά τα διαγράφουμε από το clean_text\n",
    "\n",
    "tweets['clean_text'] = tweets['clean_text'].apply(lambda tweet: ' '.join([word for word in tweet.split() if not word.startswith('#')]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f83837f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074764fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets[tweets['number_of_hashtags_included']>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798386a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.iloc[75555].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65de19e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://datagy.io/python-remove-punctuation-from-string/\n",
    "# Είναι σημαντικό να αφαιρέσουμε τα σημεία στίξης αφού πρώτα διαχειριστούμε υπερσυνδέσμους, hashtags, emojis\n",
    "# και mentions.\n",
    "def remove_punctuations(text):\n",
    "    '''\n",
    "    Με την συνάρτηση αυτή αφαιρούμε τα σημεία στίξης.\n",
    "    Η παύλα δεν αφαιρείται για να παραμείνει στο CVE αν και θα ήταν καλό να μπορούμε να αφαιρέσουμε όλες τις παύλες\n",
    "    που δεν είναι μέρος του CVE.\n",
    "    '''\n",
    "    my_punctuations = string.punctuation\n",
    "    new_text = text.translate(str.maketrans('', ' ', string.punctuation.replace('-','')))\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d607ee4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Αφαίρεση σημείων στίξης\n",
    "tweets['clean_text'] = tweets['clean_text'].apply(lambda tweet: remove_punctuations(tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2811ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# κειμενο απο αλλες γλωσσες\n",
    "# τονισμενες λεξεις\n",
    "# emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05d057d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_accented_chars(text):\n",
    "    \"\"\"Συνάρτηση για αφαίρεση των τόνων, π.χ. από café θα γίνει μετατροπή σε cafe.\n",
    "       Αυτό είναι απαραίτητο ώστε οι αλγόριθμοι να μην αντιμετωπίσουν ως ξεχωριστές λέξεις δύο ίδιες λόγω διαφορετικού \n",
    "       τονισμού.\n",
    "       SOS: αυτό αφαιρεί και emoji και κείμενο από άλλες γλωσσες και επίσης και άλλους χαρακτήρες όπως # και @ \n",
    "       οπότε αφαιρείται.\"\"\"\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4787edb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Παράδειγμα:\n",
    "x = 'Áccěntěd těxt rks Browser に複数の脆弱性 🚨 NEW: καλημέρα CVE-2021-24014 🚨 Multiple #git @github' \n",
    "x = remove_accented_chars(x)\n",
    "print(x)\n",
    "' '.join(x.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fccbb86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# υπάρχει και αυτή η επιλογή για το transiteration\n",
    "unidecode.unidecode(\"Καλημέρα!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efd6004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Αφαίρεση τονισμένων, emojis, και κειμένου από άλλες γλώσσες \n",
    "tweets[\"clean_text\"] = tweets[\"clean_text\"].apply(lambda tweet: remove_accented_chars(tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e396af58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Αφαίρεση αχρείαστων κενών (μπορεί να έχουν δημιουργηθεί από την αφαίρεση emojis...)\n",
    "tweets['clean_text'] = tweets['clean_text'].apply(lambda tweet: ' '.join(tweet.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a5e88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization \n",
    "\n",
    "tweets['clean_text'] = tweets['clean_text'].apply(lambda tweet: nltk.tokenize.word_tokenize(tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5650a032",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972e9d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Αφαίρεση stopwords\n",
    "\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "tweets['clean_text'] = tweets['clean_text'].apply(lambda tweet: [word for word in tweet if word not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea6b21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming\n",
    "\n",
    "tweets['clean_text'] = tweets['clean_text'].apply(lambda tweet: [nltk.stem.PorterStemmer().stem(word) for word in tweet])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713a68b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pipeline για το model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7793fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Απομένουν:\n",
    "\n",
    "# διαχείριση αριθμών (εκτός από αυτών που είναι σε CVE)\n",
    "# καλύτερα θα ήταν να μεταφράζουμε το κείμενο άλλων γλωσσών που δεν είναι σε λατινικούς χαρακτήρες αντί να το διαγράφουμε; \n",
    "# καλύτερα θα ήταν να \"μεταφράζουμε\" τα emojis;\n",
    "# χρειάζονται τα Common words removal  και Rare words removal  πρίν το μοντέλο; (από ινδό)\n",
    "# Spelling Correction ? (από το notebook του ινδού)\n",
    "# κάνε και ένα wordcloud τώρα που είναι φτιαγμένα τα data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f03c1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.to_csv('Data/tweets/tweets_2021_with_users_matched_with_text_preprocessed.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
